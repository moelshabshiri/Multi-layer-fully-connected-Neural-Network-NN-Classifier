# Multi-layer-fully-connected-Neural-Network-NN-Classifier
Multi-layer fully connected Neural Network (NN) Classifier of 5 classes of flower images. The classifier reached a top accuracy of 45.6%. The classifier was built using Python from scratch without the use of any frameworks such as Tensorflow or Keras.

For the data processing, I processed all images so that all images are centered, this was done by subtracting the mean images from the all images

The number of nodes I used was 200 nodes and the learning rate is 0.000005 and the number of hidden layers is 1. After changing the number of hidden layers the ACCR did not change much, it varied from the end of 30s% and beginnings of 40s% so I kept it at 1 hidden layer. I started with 50 nodes however after changing the values from to 30, 50, 100, 130, 150,200, 220 I found that 200 was the most consistent and highest in terms of ACCR. The learning rate is what affected my results the most. At 0.001, I started with 97% training accuracy and a training loss of 0.359, however the training accuracy kept on decreasing as it went through each epoch. However, the biggest was that since the Ws were too big, the scores were sometimes huge that sometimes the exponential function was not able to calculate the exponential of the losses, therefore, their were errors with the validation accuracy, it would most of the epochs stay at 20%. However after playing with the learning rate, I settled on .000005, that way the training accuracy starts at around 60% and it would increase to about 97 or 98% after a few dozen epochs. Similarly the validation accuracy started low as well at around 20% but it would increase to a maximum of between 39-46%. The same with the training losses, they decreased as the epochs went by. However with the average validation losses, they would decrease up until the first few epochs then start to increase afterwards even though the accuracy increases.

Nodes=220, Hidden Layers=1, Learning Rate=0.000005, ACCR=45.6% This combination of parameters was the best one in terms of ACCR. However larger loses on average. I stopped training at 85 epochs for two reasons. The ACCR was increasing but it was slowly increasing, and the losses was decreasing as well. So 85 epochs was reasonable. The loss values are the best loss values of a mini-batch (10 images) per epoch.

The best recorded ACCR was 45.6% for Nodes=220, Hidden Layers=1, Learning Rate=0.000005
